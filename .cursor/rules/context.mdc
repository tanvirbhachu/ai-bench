---
alwaysApply: true
---

AIBench: Project Context & Rules

1. Project Overview

AIBench is a high-fidelity, type-safe CLI tool for benchmarking Large Language Models (LLMs)

File Architecture: The entire tool runs from a monolithic index.ts file using the Bun runtime. We but we abstract the following into their own files: combining all the runs into a single json (`combine-runs.ts`) and tools the user may want to setup (`tools.ts`)

High-Fidelity TUI: Uses React Ink to provide a rich, interactive dashboard (tables, progress bars) instead of linear text logs.

Resilient: Implements a "Crash-Proof" write-ahead log strategy.

Universal: Benchmarks any model via OpenRouter using the Vercel AI SDK.

2. Technology Stack (Strict)
Runtime: bun (v1.1+) - Chosen for native TypeScript support and fast startup.

UI Framework: react + ink - For declarative TUI rendering.

Validation: zod - For strict runtime validation of all inputs (CLI args, user configs, API responses).

AI Interface: ai (Vercel AI SDK) + @openrouter/ai-sdk-provider.

CLI Parsing: cac - Lightweight, chainable argument parsing.

Concurrency: p-limit - To manage API rate limits.

Persistence: Native bun file I/O with atomic write strategies.

3. Architecture Rules

3.1 The "Monolithic Script" Structure

The index.ts file must be organized into distinct, "region-like" sections to maintain modularity within a single file. Do not split logic into separate files unless absolutely necessary for the final binary compilation. The exception here is having separate files that hold the models, tools and the code from analysing all the runs into a single json.

Section Order:

Imports & Polyfills: External deps only.

Types & Schemas: Zod definitions (BenchmarkTest, ModelConfig) and inferred TS types.

The Logic Core (Model): BenchmarkRunner class. Pure TS, no UI code. Extends EventEmitter.

The Presentation Layer (View): React components (App, Dashboard, LiveTable).

The Controller (Bootstrap): main() function that orchestrates cac parsing and mounts the React app.

3.2 Process Isolation (Crucial)
To keep the UI smooth (60fps) while heavy networking/parsing occurs:

The Main Process handles the UI rendering (React Ink).

Test Execution must occur in Subprocesses or essentially be treated as non-blocking async tasks that yield to the event loop frequently.

Note: Since we are targeting a single file, full process spawning of itself can be complex. Prefer aggressive concurrency management (p-limit) and await yielding over complex worker threads for V1, unless the UI freezes.

3.3 Type Safety Strategy
Zero any Policy: All data entering the system (from files or APIs) must be validated via Zod.

Schema Registry: User JSON benchmarks are validated against a registry of Zod schemas defined in index.ts.

Discriminated Unions: Use the mode field ('structured' | 'text') to narrow types for benchmark configurations.

3.4 Persistence Strategy (WAL)
Atomic Writes: Never write directly to results.json.

Log First: Write individual run results to runs/{model}/{timestamp}-{test}.json immediately upon completion.

Aggregate Later: The final JSON report is generated by aggregating these json files, ensuring data is saved even if the process crashes.

4. Coding Standards for Cursor
4.1 UI Development (React Ink)
Layouts: Use <Box> with Flexbox properties (flexDirection, justifyContent). Avoid hardcoded newlines.

Tables: Do NOT use cli-table3 strings inside React. Build a custom Flexbox-based table component for reactivity.

Hooks: Use useInput for interactivity (pausing, filtering).

State: Use a custom hook useBenchmarkStore to bridge the BenchmarkRunner (EventEmitter) and React state. Throttle updates to ~50ms to prevent React render thrashing.

4.2 Benchmark Logic
Structured Mode: Use generateObject from Vercel AI SDK.

Text Mode: Use generateText followed by a secondary "Judge" call (LLM-as-a-judge pattern).

Judge Prompts: Use Chain-of-Thought (CoT) in judge prompts to ensure accuracy.

4.3 Input Handling
Dynamic Imports: When loading user configuration (models.ts) or benchmarks, use import() with paths resolved relative to process.cwd().

**Validation:**typescript // Example Pattern const configFile = await import(path); const config = ConfigSchema.parse(configFile.default); // Validate immediately


5. Implementation Roadmap (Reference)
Scaffolding: Setup index.ts with regions and cac CLI definitions.

Data Layer: Define Zod schemas for BenchmarkTest, ModelConfig, and RunResult.

Runner Engine: Implement BenchmarkRunner class with p-limit and OpenRouter integration.

UI Construction: Build the Dashboard and LiveTable components using Ink.

Integration: Connect Runner events to UI state.

Persistence: Implement the Markdown logging system.

Binary Compile: Verify bun build --compile works.

6. Common Pitfalls to Avoid
Blocking the Event Loop: Do not perform synchronous file I/O in the render loop.

Zod Mismatch: Ensure the "Judge" output schema matches the Zod definition exactly.

Rate Limits: Always respect the parallel flag to avoid 429 errors from OpenRouter.

React Keys: When rendering the live table, ensure unique key props are used for rows to prevent rendering artifacts.

7. Additional Stuff and Notes from Planning Stage

- absolutely everything should be type-safe

- we have 2 modes: structured outputs and text.

- structured outputs use zod to confirm that the model can export the correct structured output

- the normal tests (because they generate text that can't be verified) will use a secondary model to verify the test. Eg. let's say the test is "What is the name of the dude that wrote `Crime and Punishment`" then we can add an appropriate answer like "fyodor dostoevsky" and the secondary model will judge the answer on whether the initial model was correct or not.

- The user can decide which model to use for the secondary `judging` model.

- We'll also track the time and token use of each run, the average usage for a model, the average for test (since multiple runs can exist for a test) and the total.